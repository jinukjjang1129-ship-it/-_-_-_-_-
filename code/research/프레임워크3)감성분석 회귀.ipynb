{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265704f-9f0a-4fc6-ab2b-2104a5b85827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트럼프 감성분석, 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e0ed19-0124-418c-b761-d26f848e1900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\shinchaewon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          dt_ny_naive                                         text_clean  \\\n",
      "0 2016-11-08 01:42:00  Today we are going to win the great state of M...   \n",
      "1 2016-11-08 06:43:00                 TODAY WE MAKE AMERICA GREAT AGAIN!   \n",
      "2 2016-11-08 11:39:00  VOTE TODAY! Go to to find your polling locatio...   \n",
      "3 2016-11-08 13:03:00  We need your vote. Go to the POLLS! Let's cont...   \n",
      "4 2016-11-08 13:23:00                                        ElectionDay   \n",
      "5 2016-11-08 16:18:00  I will be watching the election results from T...   \n",
      "6 2016-11-08 16:28:00  Just out according to : \"Utah officials report...   \n",
      "7 2016-11-08 16:31:00  Don't let up, keep getting out to vote - this ...   \n",
      "8 2016-11-08 18:03:00      Still time to VoteTrump! iVoted ElectionNight   \n",
      "9 2016-11-08 21:48:00  Watching the returns at 9:45pm. ElectionNight ...   \n",
      "\n",
      "   sentiment    close  \n",
      "0     0.9478  2127.25  \n",
      "1     0.6588  2124.75  \n",
      "2     0.6892  2134.50  \n",
      "3     0.0000  2141.00  \n",
      "4     0.0000  2138.50  \n",
      "5     0.7836  2135.25  \n",
      "6    -0.4019  2135.25  \n",
      "7     0.2810  2134.75  \n",
      "8     0.0000  2139.75  \n",
      "9     0.0000  2068.25  \n",
      "          dt_ny_naive  sentiment  ret_1min  ret_10min  ret_20min  ret_30min  \\\n",
      "0 2016-11-08 01:42:00     0.9478  0.006934   0.006346   0.007051   0.006229   \n",
      "1 2016-11-08 06:43:00     0.6588 -0.033180  -0.043417  -0.037299  -0.033886   \n",
      "2 2016-11-08 11:39:00     0.6892 -0.028110  -0.026704  -0.026704  -0.017217   \n",
      "3 2016-11-08 13:03:00     0.0000 -0.016231  -0.019851  -0.022419  -0.020201   \n",
      "4 2016-11-08 13:23:00     0.0000 -0.020926  -0.019055  -0.018120  -0.019289   \n",
      "\n",
      "   ret_45min  ret_60min  ret_120min  \n",
      "0   0.006816   0.005289    0.011047  \n",
      "1  -0.039887  -0.040122   -0.043652  \n",
      "2  -0.020848  -0.018857   -0.018154  \n",
      "3  -0.018800  -0.022186   -0.024872  \n",
      "4  -0.021277  -0.020458   -0.026537  \n",
      "\n",
      "=== horizons별 회귀 요약 (ret_h ~ sentiment) ===\n",
      "   horizon_min  n_obs  beta_sentiment  t_sentiment  p_sentiment        R2\n",
      "0            1   7681        0.000243     1.891397     0.058571  0.000499\n",
      "1           10   7514        0.000185     1.410732     0.158324  0.000284\n",
      "2           20   7501        0.000176     1.338328     0.180790  0.000255\n",
      "3           30   7423        0.000260     1.954486     0.050644  0.000552\n",
      "4           45   7446        0.000241     1.820567     0.068673  0.000476\n",
      "5           60   7396        0.000216     1.594436     0.110838  0.000374\n",
      "6          120   7290        0.000188     1.334551     0.182023  0.000260\n",
      "\n",
      "긍정 트윗 개수: 5128 부정 트윗 개수: 2758\n",
      "\n",
      "=== horizons별 t-test (positive vs negative sentiment) ===\n",
      "   horizon_min  pos_n  neg_n    t_stat   p_value  pos_mean  neg_mean\n",
      "0            1   4491   2369  2.246708  0.024707  0.000133 -0.000258\n",
      "1           10   4374   2333  1.861475  0.062742  0.000118 -0.000211\n",
      "2           20   4380   2337  1.746661  0.080764  0.000103 -0.000208\n",
      "3           30   4336   2295  2.325568  0.020087  0.000055 -0.000363\n",
      "4           45   4364   2281  2.266734  0.023455  0.000086 -0.000322\n",
      "5           60   4296   2313  1.813646  0.069800 -0.000035 -0.000365\n",
      "6          120   4224   2284  1.859219  0.063062 -0.000039 -0.000392\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "path_tweets = r\"C:\\Users\\shinchaewon\\Desktop\\텍스트마이닝\\trump_tweets_2016_2021_final.csv\"\n",
    "path_spx    = r\"C:\\Users\\shinchaewon\\Desktop\\텍스트마이닝\\SPXUSD_filtered.xlsx\"\n",
    "\n",
    "horizons = [1, 10, 20, 30, 45, 60, 120]\n",
    "\n",
    "start_date = pd.Timestamp(\"2016-11-08\", tz=\"UTC\")\n",
    "end_date   = pd.Timestamp(\"2019-09-22 23:59:59\", tz=\"UTC\")  # inclusive\n",
    "\n",
    "# VADER 친화 전처리 (신호 보존)\n",
    "#   - 유지: 대문자, !!!, ???, 반복부호, 조동사, 강조부사, 리듬, 이모지/구두점 대부분\n",
    "#   - 제거: URL, @멘션, RT(리트윗 토큰), (옵션) 앞뒤 공백만 정리\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "url_pat = re.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re.IGNORECASE)\n",
    "mention_pat = re.compile(r\"@\\w+\")\n",
    "# RT를 문장 내 의미단어로 보지 않게 제거 (대문자/리듬 유지 위해 과격한 정규화 X)\n",
    "rt_pat = re.compile(r\"(^|\\s)RT(\\s|:)\", flags=re.IGNORECASE)\n",
    "# 공백만 정리 (구두점/대문자/반복부호는 그대로 둠)\n",
    "multi_space_pat = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_for_vader(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    t = str(text)\n",
    "\n",
    "    # URL 제거 (VADER에 의미 없음)\n",
    "    t = url_pat.sub(\" \", t)\n",
    "\n",
    "    # 멘션 제거 (사용자 핸들은 의미 거의 없음)\n",
    "    t = mention_pat.sub(\" \", t)\n",
    "\n",
    "    # RT 토큰만 제거 (리듬/구두점 최대 보존)\n",
    "    t = rt_pat.sub(\" \", t)\n",
    "\n",
    "    # 해시태그는 #만 빼고 단어는 살림 (감성 단어일 수 있음)\n",
    "    # 예: \"#GREAT\" -> \"GREAT\"\n",
    "    t = t.replace(\"#\", \"\")\n",
    "\n",
    "    # 공백만 정리 (문장 부호/대문자/반복부호 유지)\n",
    "    t = multi_space_pat.sub(\" \", t).strip()\n",
    "\n",
    "    return t\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if pd.isna(text) or str(text).strip() == \"\":\n",
    "        return 0.0\n",
    "    return sid.polarity_scores(str(text))[\"compound\"]\n",
    "\n",
    "\n",
    "# 1) 트윗 로드 + 기간 필터 + VADER 감성\n",
    "df = pd.read_csv(path_tweets, encoding=\"utf-8\")\n",
    "\n",
    "df[\"datetime_utc\"] = pd.to_datetime(df[\"datetime_utc\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"datetime_utc\"]).copy()\n",
    "\n",
    "# 항상 UTC tz-aware로 통일\n",
    "df[\"datetime_utc\"] = pd.to_datetime(df[\"datetime_utc\"], utc=True)\n",
    "\n",
    "# 논문 기간 필터 (UTC 기준)\n",
    "df = df[(df[\"datetime_utc\"] >= start_date) & (df[\"datetime_utc\"] <= end_date)].copy()\n",
    "\n",
    "# VADER 친화 전처리 + 감성\n",
    "df[\"text_clean\"] = df[\"text\"].astype(str).apply(clean_for_vader)\n",
    "df[\"sentiment\"] = df[\"text_clean\"].apply(get_sentiment)\n",
    "\n",
    "# UTC → NY (tz-aware)\n",
    "df[\"dt_ny\"] = df[\"datetime_utc\"].dt.tz_convert(\"America/New_York\")\n",
    "\n",
    "# merge용 naive\n",
    "df[\"dt_ny_naive\"] = df[\"dt_ny\"].dt.tz_localize(None)\n",
    "\n",
    "# 2) SPX 분봉 로드 + 기간 필터 + 정렬\n",
    "spx = pd.read_excel(path_spx)\n",
    "\n",
    "spx[\"datetime\"] = pd.to_datetime(spx[\"datetime\"], errors=\"coerce\")\n",
    "spx = spx.dropna(subset=[\"datetime\"]).copy()\n",
    "spx = spx.rename(columns={\"datetime\": \"dt_ny_naive\"})\n",
    "\n",
    "start_ny_naive = start_date.tz_convert(\"America/New_York\").tz_localize(None)\n",
    "end_ny_naive   = end_date.tz_convert(\"America/New_York\").tz_localize(None)\n",
    "\n",
    "spx = spx[(spx[\"dt_ny_naive\"] >= start_ny_naive) & (spx[\"dt_ny_naive\"] <= end_ny_naive)].copy()\n",
    "\n",
    "df = df.sort_values(\"dt_ny_naive\")\n",
    "spx = spx.sort_values(\"dt_ny_naive\")\n",
    "\n",
    "\n",
    "# 3) 트윗에 현재 close 붙이기 (backward)\n",
    "merged = pd.merge_asof(\n",
    "    df,\n",
    "    spx[[\"dt_ny_naive\", \"close\"]],\n",
    "    on=\"dt_ny_naive\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "\n",
    "print(merged[[\"dt_ny_naive\", \"text_clean\", \"sentiment\", \"close\"]].head(10))\n",
    "\n",
    "# 4) horizons별 \"h분 뒤\" 수익률 만들기\n",
    "spx_base = spx[[\"dt_ny_naive\", \"close\"]].copy().sort_values(\"dt_ny_naive\")\n",
    "spx_base = spx_base.rename(columns={\"close\": \"close_t\"})\n",
    "\n",
    "spx_lookup = spx[[\"dt_ny_naive\", \"close\"]].copy().sort_values(\"dt_ny_naive\")\n",
    "spx_lookup = spx_lookup.rename(columns={\"close\": \"close_future\"})\n",
    "\n",
    "for h in horizons:\n",
    "    tmp = spx_base.copy()\n",
    "    tmp[f\"dt_plus_{h}\"] = tmp[\"dt_ny_naive\"] + pd.Timedelta(minutes=h)\n",
    "\n",
    "    tmp = tmp.merge(\n",
    "        spx_lookup.rename(columns={\"dt_ny_naive\": f\"dt_plus_{h}\"}),\n",
    "        on=f\"dt_plus_{h}\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    tmp = tmp.rename(columns={\"close_future\": f\"close_t_plus_{h}\"})\n",
    "    spx_base[f\"close_t_plus_{h}\"] = tmp[f\"close_t_plus_{h}\"]\n",
    "    spx_base[f\"ret_{h}min\"] = spx_base[f\"close_t_plus_{h}\"] / spx_base[\"close_t\"] - 1\n",
    "\n",
    "# 5) 트윗에 horizons별 수익률 붙이기 (backward)\n",
    "merged_ret = pd.merge_asof(\n",
    "    df.sort_values(\"dt_ny_naive\"),\n",
    "    spx_base.sort_values(\"dt_ny_naive\"),\n",
    "    on=\"dt_ny_naive\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "\n",
    "print(merged_ret[[\"dt_ny_naive\", \"sentiment\"] + [f\"ret_{h}min\" for h in horizons]].head())\n",
    "\n",
    "# 6) horizons별 회귀: ret_h ~ sentiment (HC3)\n",
    "reg_rows = []\n",
    "for h in horizons:\n",
    "    ycol = f\"ret_{h}min\"\n",
    "    tmp = merged_ret.dropna(subset=[ycol, \"sentiment\"]).copy()\n",
    "\n",
    "    X = sm.add_constant(tmp[\"sentiment\"])\n",
    "    y = tmp[ycol]\n",
    "    model = sm.OLS(y, X).fit(cov_type=\"HC3\")\n",
    "\n",
    "    reg_rows.append({\n",
    "        \"horizon_min\": h,\n",
    "        \"n_obs\": int(model.nobs),\n",
    "        \"beta_sentiment\": float(model.params[\"sentiment\"]),\n",
    "        \"t_sentiment\": float(model.tvalues[\"sentiment\"]),\n",
    "        \"p_sentiment\": float(model.pvalues[\"sentiment\"]),\n",
    "        \"R2\": float(model.rsquared),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(reg_rows).sort_values(\"horizon_min\")\n",
    "print(\"\\n=== horizons별 회귀 요약 (ret_h ~ sentiment) ===\")\n",
    "print(results_df)\n",
    "\n",
    "# 7) horizons별 t-test: sentiment>0 vs sentiment<0\n",
    "tt_rows = []\n",
    "pos = merged_ret[merged_ret[\"sentiment\"] > 0]\n",
    "neg = merged_ret[merged_ret[\"sentiment\"] < 0]\n",
    "\n",
    "print(\"\\n긍정 트윗 개수:\", len(pos), \"부정 트윗 개수:\", len(neg))\n",
    "\n",
    "for h in horizons:\n",
    "    ycol = f\"ret_{h}min\"\n",
    "    pos_h = pos[ycol].dropna()\n",
    "    neg_h = neg[ycol].dropna()\n",
    "\n",
    "    if len(pos_h) < 2 or len(neg_h) < 2:\n",
    "        tt_rows.append({\n",
    "            \"horizon_min\": h,\n",
    "            \"pos_n\": len(pos_h),\n",
    "            \"neg_n\": len(neg_h),\n",
    "            \"t_stat\": np.nan,\n",
    "            \"p_value\": np.nan,\n",
    "            \"pos_mean\": pos_h.mean() if len(pos_h) else np.nan,\n",
    "            \"neg_mean\": neg_h.mean() if len(neg_h) else np.nan,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    t_stat, p_val = ttest_ind(pos_h, neg_h, equal_var=False)\n",
    "    tt_rows.append({\n",
    "        \"horizon_min\": h,\n",
    "        \"pos_n\": len(pos_h),\n",
    "        \"neg_n\": len(neg_h),\n",
    "        \"t_stat\": float(t_stat),\n",
    "        \"p_value\": float(p_val),\n",
    "        \"pos_mean\": float(pos_h.mean()),\n",
    "        \"neg_mean\": float(neg_h.mean()),\n",
    "    })\n",
    "\n",
    "tt_df = pd.DataFrame(tt_rows).sort_values(\"horizon_min\")\n",
    "print(\"\\n=== horizons별 t-test (positive vs negative sentiment) ===\")\n",
    "print(tt_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd00ba4b-1aa6-4f5b-940c-1d78be33bbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2017 horizons별 회귀 요약 (ret_h ~ sentiment) ===\n",
      "   horizon_min  n_obs  beta_sentiment  t_sentiment  p_sentiment        R2\n",
      "0            1   1486        0.001540     2.004238     0.045045  0.002739\n",
      "1           10   1458        0.001410     1.833874     0.066673  0.002317\n",
      "2           20   1405        0.001084     1.402843     0.160664  0.001401\n",
      "3           30   1406        0.001231     1.620329     0.105162  0.001863\n",
      "4           45   1373        0.001441     1.864339     0.062274  0.002544\n",
      "5           60   1458        0.001704     2.332269     0.019687  0.003612\n",
      "6          120   1185        0.000776     0.955712     0.339218  0.000756\n",
      "\n",
      "[2017] 긍정 트윗 개수: 1336 부정 트윗 개수: 668\n",
      "\n",
      "=== 2017 horizons별 t-test (positive vs negative sentiment) ===\n",
      "   horizon_min  pos_n  neg_n    t_stat   p_value  pos_mean  neg_mean\n",
      "0            1    855    426  1.770047  0.077072  0.028892  0.027170\n",
      "1           10    811    439  1.786405  0.074363  0.028637  0.026911\n",
      "2           20    799    414  1.319130  0.187482  0.028925  0.027625\n",
      "3           30    790    419  1.389762  0.164961  0.029236  0.027901\n",
      "4           45    787    399  2.085828  0.037306  0.029026  0.027007\n",
      "5           60    812    447  1.710768  0.087453  0.028549  0.026969\n",
      "6          120    681    344  0.909738  0.363269  0.029694  0.028746\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 2017년\n",
    "# ================================\n",
    "\n",
    "horizons = [1, 10, 20, 30, 45, 60, 120]\n",
    "\n",
    "start_2017 = pd.Timestamp(\"2017-01-01 00:00:00\", tz=\"UTC\")\n",
    "end_2017   = pd.Timestamp(\"2017-12-31 23:59:59\", tz=\"UTC\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) 트윗: 2017년만 (UTC 기준) 필터\n",
    "# ----------------------------\n",
    "df_2017 = df.copy()\n",
    "\n",
    "df_2017[\"datetime_utc\"] = pd.to_datetime(df_2017[\"datetime_utc\"], utc=True, errors=\"coerce\")\n",
    "df_2017 = df_2017.dropna(subset=[\"datetime_utc\"]).copy()\n",
    "\n",
    "df_2017 = df_2017[(df_2017[\"datetime_utc\"] >= start_2017) & (df_2017[\"datetime_utc\"] <= end_2017)].copy()\n",
    "\n",
    "if \"dt_ny_naive\" not in df_2017.columns:\n",
    "    df_2017[\"dt_ny\"] = df_2017[\"datetime_utc\"].dt.tz_convert(\"America/New_York\")\n",
    "    df_2017[\"dt_ny_naive\"] = df_2017[\"dt_ny\"].dt.tz_localize(None)\n",
    "\n",
    "# 감성(전처리 + vader)\n",
    "df_2017[\"text_clean\"] = df_2017[\"text\"].astype(str).apply(clean_for_vader)\n",
    "df_2017[\"sentiment\"] = df_2017[\"text_clean\"].apply(get_sentiment)\n",
    "\n",
    "df_2017 = df_2017.sort_values(\"dt_ny_naive\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) SPX: 2017년만 (NY naive 기준) 필터\n",
    "# ----------------------------\n",
    "spx_2017 = spx.copy()\n",
    "spx_2017[\"dt_ny_naive\"] = pd.to_datetime(spx_2017[\"dt_ny_naive\"], errors=\"coerce\")\n",
    "spx_2017 = spx_2017.dropna(subset=[\"dt_ny_naive\"]).copy()\n",
    "\n",
    "start_ny = start_2017.tz_convert(\"America/New_York\").tz_localize(None)\n",
    "end_ny   = end_2017.tz_convert(\"America/New_York\").tz_localize(None)\n",
    "\n",
    "spx_2017 = spx_2017[(spx_2017[\"dt_ny_naive\"] >= start_ny) & (spx_2017[\"dt_ny_naive\"] <= end_ny)].copy()\n",
    "spx_2017 = spx_2017.sort_values(\"dt_ny_naive\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3)트윗에 현재 close 붙이기\n",
    "merged_2017 = pd.merge_asof(\n",
    "    df_2017,\n",
    "    spx_2017[[\"dt_ny_naive\", \"close\"]],\n",
    "    on=\"dt_ny_naive\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) 2017년 ret_{h}min 만들기\n",
    "spx_base_2017 = spx_2017[[\"dt_ny_naive\", \"close\"]].copy().sort_values(\"dt_ny_naive\")\n",
    "spx_base_2017 = spx_base_2017.rename(columns={\"close\": \"close_t\"})\n",
    "\n",
    "spx_lookup_2017 = spx_2017[[\"dt_ny_naive\", \"close\"]].copy().sort_values(\"dt_ny_naive\")\n",
    "spx_lookup_2017 = spx_lookup_2017.rename(columns={\"close\": \"close_future\"})\n",
    "\n",
    "for h in horizons:\n",
    "    tmp = spx_base_2017[[\"dt_ny_naive\", \"close_t\"]].copy()\n",
    "    tmp[f\"dt_plus_{h}\"] = tmp[\"dt_ny_naive\"] + pd.Timedelta(minutes=h)\n",
    "\n",
    "    tmp = tmp.merge(\n",
    "        spx_lookup_2017.rename(columns={\"dt_ny_naive\": f\"dt_plus_{h}\"}),\n",
    "        on=f\"dt_plus_{h}\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    spx_base_2017[f\"close_t_plus_{h}\"] = tmp[\"close_future\"]\n",
    "    spx_base_2017[f\"ret_{h}min\"] = spx_base_2017[f\"close_t_plus_{h}\"] / spx_base_2017[\"close_t\"] - 1\n",
    "\n",
    "# ----------------------------\n",
    "# 5) 트윗에 2017 ret 붙이기\n",
    "merged_ret_2017 = pd.merge_asof(\n",
    "    df_2017.sort_values(\"dt_ny_naive\"),\n",
    "    spx_base_2017.sort_values(\"dt_ny_naive\"),\n",
    "    on=\"dt_ny_naive\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) 2017 회귀: ret_h ~ sentiment (HC3)\n",
    "reg_rows = []\n",
    "for h in horizons:\n",
    "    ycol = f\"ret_{h}min\"\n",
    "    tmp = merged_ret_2017.dropna(subset=[ycol, \"sentiment\"]).copy()\n",
    "\n",
    "    X = sm.add_constant(tmp[\"sentiment\"])\n",
    "    y = tmp[ycol]\n",
    "    model = sm.OLS(y, X).fit(cov_type=\"HC3\")\n",
    "\n",
    "    reg_rows.append({\n",
    "        \"horizon_min\": h,\n",
    "        \"n_obs\": int(model.nobs),\n",
    "        \"beta_sentiment\": float(model.params[\"sentiment\"]),\n",
    "        \"t_sentiment\": float(model.tvalues[\"sentiment\"]),\n",
    "        \"p_sentiment\": float(model.pvalues[\"sentiment\"]),\n",
    "        \"R2\": float(model.rsquared),\n",
    "    })\n",
    "\n",
    "results_2017 = pd.DataFrame(reg_rows).sort_values(\"horizon_min\")\n",
    "print(\"\\n=== 2017 horizons별 회귀 요약 (ret_h ~ sentiment) ===\")\n",
    "print(results_2017)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) 2017 t-test: sentiment>0 vs sentiment<0\n",
    "tt_rows = []\n",
    "pos = merged_ret_2017[merged_ret_2017[\"sentiment\"] > 0]\n",
    "neg = merged_ret_2017[merged_ret_2017[\"sentiment\"] < 0]\n",
    "\n",
    "print(\"\\n[2017] 긍정 트윗 개수:\", len(pos), \"부정 트윗 개수:\", len(neg))\n",
    "\n",
    "for h in horizons:\n",
    "    ycol = f\"ret_{h}min\"\n",
    "    pos_h = pos[ycol].dropna()\n",
    "    neg_h = neg[ycol].dropna()\n",
    "\n",
    "    if len(pos_h) < 2 or len(neg_h) < 2:\n",
    "        tt_rows.append({\n",
    "            \"horizon_min\": h,\n",
    "            \"pos_n\": len(pos_h),\n",
    "            \"neg_n\": len(neg_h),\n",
    "            \"t_stat\": np.nan,\n",
    "            \"p_value\": np.nan,\n",
    "            \"pos_mean\": pos_h.mean() if len(pos_h) else np.nan,\n",
    "            \"neg_mean\": neg_h.mean() if len(neg_h) else np.nan,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    t_stat, p_val = ttest_ind(pos_h, neg_h, equal_var=False)\n",
    "    tt_rows.append({\n",
    "        \"horizon_min\": h,\n",
    "        \"pos_n\": len(pos_h),\n",
    "        \"neg_n\": len(neg_h),\n",
    "        \"t_stat\": float(t_stat),\n",
    "        \"p_value\": float(p_val),\n",
    "        \"pos_mean\": float(pos_h.mean()),\n",
    "        \"neg_mean\": float(neg_h.mean()),\n",
    "    })\n",
    "\n",
    "tt_2017 = pd.DataFrame(tt_rows).sort_values(\"horizon_min\")\n",
    "print(\"\\n=== 2017 horizons별 t-test (positive vs negative sentiment) ===\")\n",
    "print(tt_2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80490a92-6ea5-4ac1-be44-ac4f47135aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
